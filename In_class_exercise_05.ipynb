{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "In-class-exercise-05.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VijayaBhargavi198/5731Assignments/blob/master/In_class_exercise_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prJCuoIfqowv"
      },
      "source": [
        "## The fifth In-class-exercise (9/30/2020, 20 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTljjISgqowx"
      },
      "source": [
        "In exercise-03, I asked you to collected 500 textual data based on your own information needs (If you didn't collect the textual data, you should recollect for this exercise). Now we need to think about how to represent the textual data for text classification. In this exercise, you are required to select 10 types of features (10 types of features but absolutely more than 10 features) in the followings feature list, then represent the 500 texts with these features. The output should be in the following format:\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "The feature list:\n",
        "\n",
        "* (1) tf-idf features\n",
        "* (2) POS-tag features: number of adjective, adverb, auxiliary, punctuation, complementizer, coordinating conjunction, subordinating conjunction, determiner, interjection, noun, possessor, preposition, pronoun, quantifier, verb, and other. (select some of them if you use pos-tag features)\n",
        "* (3) Linguistic features:\n",
        "  * number of right-branching nodes across all constituent types\n",
        "  * number of right-branching nodes for NPs only\n",
        "  * number of left-branching nodes across all constituent types\n",
        "  * number of left-branching nodes for NPs only\n",
        "  * number of premodifiers across all constituent types\n",
        "  * number of premodifiers within NPs only\n",
        "  * number of postmodifiers across all constituent types\n",
        "  * number of postmodifiers within NPs only\n",
        "  * branching index across all constituent types, i.e. the number of right-branching nodes minus number of left-branching nodes\n",
        "  * branching index for NPs only\n",
        "  * branching weight index: number of tokens covered by right-branching nodes minus number of tokens covered by left-branching nodes across all categories\n",
        "  * branching weight index for NPs only \n",
        "  * modification index, i.e. the number of premodifiers minus the number of postmodifiers across all categories\n",
        "  * modification index for NPs only\n",
        "  * modification weight index: length in tokens of all premodifiers minus length in tokens of all postmodifiers across all categories\n",
        "  * modification weight index for NPs only\n",
        "  * coordination balance, i.e. the maximal length difference in coordinated constituents\n",
        "  \n",
        "  * density (density can be calculated using the ratio of folowing function words to content words) of determiners/quantifiers\n",
        "  * density of pronouns\n",
        "  * density of prepositions\n",
        "  * density of punctuation marks, specifically commas and semicolons\n",
        "  * density of auxiliary verbs\n",
        "  * density of conjunctions\n",
        "  * density of different pronoun types: Wh, 1st, 2nd, and 3rd person pronouns\n",
        "  \n",
        "  * maximal and average NP length\n",
        "  * maximal and average AJP length\n",
        "  * maximal and average PP length\n",
        "  * maximal and average AVP length\n",
        "  * sentence length\n",
        "\n",
        "* Other features in your mind (ie., pre-defined patterns)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx0_tFNoqowx"
      },
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium\n",
        "!pip install textblob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_pldJvsquOz"
      },
      "source": [
        "# Webscrap and collect data using selenium\n",
        "from selenium import webdriver\n",
        "import time\n",
        "import pandas as pd\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "# open it, go to a website, and get results\n",
        "driver = webdriver.Chrome('chromedriver',options=options)\n",
        "driver.get(\"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\") \n",
        "for i in range(0,4):\n",
        "  button =driver.find_element_by_xpath('//*[@id = \"load-more-trigger\"]').click()\n",
        "  time.sleep(5)\n",
        "output = list()\n",
        "reviews = driver.find_elements_by_class_name(\"content\")\n",
        "for review in reviews:\n",
        "  output.append(review.find_element_by_css_selector(\".text.show-more__control\").text)\n",
        "  pd.DataFrame(output,columns = [\"review\"]).to_csv(\"User_reviews\")\n",
        "#create data frame for all the reviews for data cleaning\n",
        "result_df= pd.DataFrame(output,columns = [\"User_Review\"])\n",
        "#result_df"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CmKH0m6q4_k",
        "outputId": "b98cdb3e-8a22-4e08-c841-5cbd2feb132d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "#Data Cleaning steps:\n",
        "#1.Remove noise, such as special characters and punctuations.\n",
        "#2.Remove numbers\n",
        "#3.Remove stopwords by using the stopwords list\n",
        "#4.Lowercase all texts\n",
        "#5.Stemming\n",
        "#6.Lemmatization.\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "stemmer=PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "stop_words=stopwords.words(\"english\")\n",
        "review_filter_stemming=[]\n",
        "review_filter_Lemmatization=[]\n",
        "final_stemmed_review=[]\n",
        "final_lemmatized_review=[]\n",
        "# cleaning the text data\n",
        "result_df['User_Review'] = result_df['User_Review'].str.replace(r\"\\W\", \" \").str.strip()# 1.To remove special characters and punctuations\n",
        "result_df['User_Review']= result_df['User_Review'].str.replace(r'\\d+',\"\") #2.To remove Numbers\n",
        "for a in result_df['User_Review']:\n",
        "    splitting_words=word_tokenize(a)\n",
        "    for b in splitting_words:\n",
        "        if b not in stop_words: #3.Removing Stop Words\n",
        "            b=b.lower() # 4.Coverting text to lower case \n",
        "            stemming=stemmer.stem(b) # 5.perfroming stemming\n",
        "            review_filter_stemming.append(stemming)\n",
        "            lemmatization_words=lemmatizer.lemmatize(b.lower()) # 6.Lemmatization\n",
        "            review_filter_Lemmatization.append(lemmatization_words)\n",
        "    final_stemmed_review.append(' '.join(review_filter_stemming))\n",
        "    final_lemmatized_review.append(' '.join(review_filter_Lemmatization))\n",
        "    review_filter_stemming.clear()\n",
        "    review_filter_Lemmatization.clear()\n",
        "result_df['Review after Stemming']=pd.DataFrame(final_stemmed_review)\n",
        "result_df['Review after Lemmatization']=pd.DataFrame(final_lemmatized_review)\n",
        "result_df['word_count'] = result_df[\"Review after Lemmatization\"].apply(lambda x : len(x.split(\" \")))\n",
        "#result_df\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXacajbcrA1P"
      },
      "source": [
        "# function definition to compute the tf idf and tf-idf values for the starting word in the text\n",
        "\n",
        "#computeTF\n",
        "def computeTF(sentence):\n",
        "  words = sentence.split(\" \")\n",
        "  value = len(set(words))\n",
        "  return words.count(words[0])/value\n",
        "  pass\n",
        "\n",
        "#compute IDF\n",
        "import math \n",
        "def computeIDF(sentence):\n",
        "  words = sentence.lower().split(\" \")\n",
        "  res = words.count(words[0])\n",
        "  return math.log(len(words)/res, 10)\n",
        "  pass"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2kq2GjSrWwg"
      },
      "source": [
        "result_df[\"tf\"] = result_df[\"Review after Lemmatization\"].apply(lambda x : round(computeTF(x),4))\n",
        "result_df[\"idf\"] = result_df[\"Review after Lemmatization\"].apply(lambda x : round(computeIDF(x),4))\n",
        "result_df[\"tf-idf\"] = result_df[\"tf\"]*result_df[\"idf\"]\n",
        "#result_df"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW6Q8vh8rZlB",
        "outputId": "db9270ff-ffaf-4bc6-b2af-305a6119d2c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# syntax and structure analysis\n",
        "#3.1 Parts of Speech (POS) Tagging: \n",
        "import nltk\n",
        "from collections import Counter\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "#initialising\n",
        "tags=[]\n",
        "count_tag=[]\n",
        "count=0\n",
        "# POS tagging\n",
        "for val in result_df['Review after Lemmatization']:\n",
        "    word_tokens=word_tokenize(val)\n",
        "    POS_tags=nltk.pos_tag(word_tokens)\n",
        "    tags.append(POS_tags)\n",
        "from collections import Counter\n",
        "def get_pos(value,tags):\n",
        "  pos_counts=[]\n",
        "  for j in tags:\n",
        "    counts = Counter(tag for word,tag in j)   \n",
        "    pos_counts.append(counts.get(value))\n",
        "  return pos_counts\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VFjXklwrc06"
      },
      "source": [
        "import numpy as np\n",
        "result_df[\"Number of nouns\"] = pd.DataFrame(get_pos(\"NN\",tags))\n",
        "result_df[\"Number of verbs\"] = pd.DataFrame(get_pos(\"VB\",tags))\n",
        "result_df[\"Number of predeterminers\"] = pd.DataFrame(get_pos(\"PDT\",tags))\n",
        "result_df[\"Number of adverbs\"] = pd.DataFrame(get_pos(\"RB\",tags))\n",
        "result_df[\"Number of cardinal digits\"] = pd.DataFrame(get_pos(\"CD\", tags))\n",
        "result_df[\"Number of prepositions\"] = pd.DataFrame(get_pos(\"IN\",tags))\n",
        "result_df[\"Number of Determiners\"] = pd.DataFrame(get_pos(\"DT\",tags)) \n",
        "result_df[\"Number of adjectives\"] = pd.DataFrame(get_pos(\"JJ\",tags)) \n",
        "result_df[\"Number of conjunctions\"] = pd.DataFrame(get_pos(\"CC\",tags))\n",
        "result_df[\"Number of pronouns\"] = pd.DataFrame(get_pos(\"PP\",tags))\n",
        "result_df[\"Number of Punctuations\"] = pd.DataFrame(get_pos(\":\",tags))\n",
        "result_df = result_df.fillna(0)\n",
        "pronouns = pd.DataFrame(get_pos(\"PP\",tags))\n",
        "prepositions = pd.DataFrame(get_pos(\"IN\",tags))\n",
        "punctuations = pd.DataFrame(get_pos(\":\",tags))\n",
        "conjunctions = pd.DataFrame(get_pos(\"CC\",tags))\n",
        "result_df[\"Density of pronouns\"] = result_df[\"Number of pronouns\"]/pronouns[0]\n",
        "result_df[\"Density of prepositions\"] = result_df[\"Number of prepositions\"]/prepositions[0]\n",
        "result_df[\"Density of punctuations\"] = result_df[\"Number of Punctuations\"]/punctuations[0]\n",
        "result_df[\"Density of conjunctions\"] = result_df[\"Number of conjunctions\"]/conjunctions[0]\n",
        "result_df = result_df.fillna(0)\n",
        "#result_df"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNEiLaZ0ro6_",
        "outputId": "d7015b59-78ea-4850-853b-86614c3a580b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        }
      },
      "source": [
        "final_df = result_df[['User_Review','word_count','tf','idf','tf-idf','Number of nouns','Number of adverbs','Number of cardinal digits','Number of verbs','Number of Determiners','Number of adjectives',\"Density of pronouns\",\"Density of prepositions\",\"Density of punctuations\",\"Density of conjunctions\"]]\n",
        "final_df"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User_Review</th>\n",
              "      <th>word_count</th>\n",
              "      <th>tf</th>\n",
              "      <th>idf</th>\n",
              "      <th>tf-idf</th>\n",
              "      <th>Number of nouns</th>\n",
              "      <th>Number of adverbs</th>\n",
              "      <th>Number of cardinal digits</th>\n",
              "      <th>Number of verbs</th>\n",
              "      <th>Number of Determiners</th>\n",
              "      <th>Number of adjectives</th>\n",
              "      <th>Density of pronouns</th>\n",
              "      <th>Density of prepositions</th>\n",
              "      <th>Density of punctuations</th>\n",
              "      <th>Density of conjunctions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Every once in a while a movie comes  that trul...</td>\n",
              "      <td>50</td>\n",
              "      <td>0.0208</td>\n",
              "      <td>1.6990</td>\n",
              "      <td>0.035339</td>\n",
              "      <td>24.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This is a movie that only those who have felt ...</td>\n",
              "      <td>43</td>\n",
              "      <td>0.0270</td>\n",
              "      <td>1.6335</td>\n",
              "      <td>0.044104</td>\n",
              "      <td>15.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Truly a masterpiece  The Best Hollywood film o...</td>\n",
              "      <td>86</td>\n",
              "      <td>0.0299</td>\n",
              "      <td>1.6335</td>\n",
              "      <td>0.048842</td>\n",
              "      <td>29.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Joaquin Phoenix gives a tour de force performa...</td>\n",
              "      <td>54</td>\n",
              "      <td>0.0222</td>\n",
              "      <td>1.7324</td>\n",
              "      <td>0.038459</td>\n",
              "      <td>24.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Most of the time movies are anticipated like t...</td>\n",
              "      <td>52</td>\n",
              "      <td>0.0213</td>\n",
              "      <td>1.7160</td>\n",
              "      <td>0.036551</td>\n",
              "      <td>18.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>My husband and I went to see this film and fou...</td>\n",
              "      <td>253</td>\n",
              "      <td>0.0051</td>\n",
              "      <td>2.4031</td>\n",
              "      <td>0.012256</td>\n",
              "      <td>94.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>JOKER is a gift to the audiences  I felt as a ...</td>\n",
              "      <td>27</td>\n",
              "      <td>0.0385</td>\n",
              "      <td>1.4314</td>\n",
              "      <td>0.055109</td>\n",
              "      <td>15.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>Dark  depressing and unsettling film with a ha...</td>\n",
              "      <td>17</td>\n",
              "      <td>0.0588</td>\n",
              "      <td>1.2304</td>\n",
              "      <td>0.072348</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>125 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           User_Review  ...  Density of conjunctions\n",
              "0    Every once in a while a movie comes  that trul...  ...                      0.0\n",
              "1    This is a movie that only those who have felt ...  ...                      1.0\n",
              "2    Truly a masterpiece  The Best Hollywood film o...  ...                      1.0\n",
              "3    Joaquin Phoenix gives a tour de force performa...  ...                      0.0\n",
              "4    Most of the time movies are anticipated like t...  ...                      0.0\n",
              "..                                                 ...  ...                      ...\n",
              "120  My husband and I went to see this film and fou...  ...                      1.0\n",
              "121  JOKER is a gift to the audiences  I felt as a ...  ...                      0.0\n",
              "122  Dark  depressing and unsettling film with a ha...  ...                      0.0\n",
              "123                                                     ...                      0.0\n",
              "124                                                     ...                      0.0\n",
              "\n",
              "[125 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRP5iTc_rrwj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}