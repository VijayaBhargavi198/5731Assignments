{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "In_class_exercise_04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VijayaBhargavi198/5731Assignments/blob/master/In_class_exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuX00KHNeSpw"
      },
      "source": [
        "# **The fourth in-class-exercise (20 points in total)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-vTOb03hG1f"
      },
      "source": [
        "# 1. Text Data Preprocessing\n",
        "\n",
        "Here is a [legal case](https://github.com/unt-iialab/INFO5731_FALL2020/blob/master/In_class_exercise/01-05-1%20%20Adams%20v%20Tanner.txt) we collected from westlaw, please follow the steps we mentioned in lesson 5 to clean the data:\n",
        "\n",
        "\n",
        "\n",
        "## 1.1 Basic feature extraction using text data (4 points)\n",
        "\n",
        "*   Number of sentences\n",
        "*   Number of words\n",
        "*   Number of characters\n",
        "*   Average word length\n",
        "*   Number of stopwords\n",
        "*   Number of special characters\n",
        "*   Number of numerics\n",
        "*   Number of uppercase words\n",
        "\n",
        "## 1.2 Basic Text Pre-processing of text data (4 points)\n",
        "\n",
        "*   Lower casing\n",
        "*   Punctuation removal\n",
        "*   Stopwords removal\n",
        "*   Frequent words removal\n",
        "*   Rare words removal\n",
        "*   Spelling correction\n",
        "*   Tokenization\n",
        "*   Stemming\n",
        "*   Lemmatization\n",
        "\n",
        "## 1.3 Save all the **clean sentences** to a **csv file** (one column, each raw is a sentence) after finishing all the steps above. (4 points)\n",
        "\n",
        "\n",
        "## 1.4 Advance Text Processing (Extra credit: 4 points)\n",
        "\n",
        "*   Calculate the term frequency of all the terms.\n",
        "*   Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd_kJkCjQENZ"
      },
      "source": [
        "!pip install pysbd\n",
        "!pip install spacy\n",
        "!pip install SpellChecker \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5Gmt06pNK7V"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pelg5T7ANI5C"
      },
      "source": [
        "import nltk\n",
        "nltk.download('Users')\n",
        "nltk.download('maxent_treebank_pos_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn823A6mLUKS",
        "outputId": "6669905f-013c-4b4e-97b8-331c7ca0d9d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        }
      },
      "source": [
        "#Text Data Preprocessing\n",
        "#following are the finding for the whole file\n",
        "\n",
        "folder = nltk.data.find('/content/sample_data/')\n",
        "corpusReader = nltk.corpus.PlaintextCorpusReader(folder, 'AdamsVsTanner.txt')\n",
        "print(corpusReader)\n",
        "print (\"Number of sentences =\", len(corpusReader.sents()))\n",
        "print (\"Number of words =\", len([word for sentence in corpusReader.sents() for word in sentence]))\n",
        "print (\"Number of characters =\", len([char for sentence in corpusReader.sents() for word in sentence for char in word]))\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "file=open('/content/sample_data/AdamsVsTanner.txt',encoding='utf-8')\n",
        "data=file.read()\n",
        "words = data.split()\n",
        "word_average = sum(len(word) for word in words) / len(words)\n",
        "print(\"Average word length:\", word_average)\n",
        "\n",
        "\n",
        "stoplist = stopwords.words('english')\n",
        "count=0\n",
        "for word in data.split():\n",
        "    if word in stoplist:\n",
        "        count +=1\n",
        "print (\"The number of stop words =\",count)\n",
        "\n",
        "punctuation = ['~', ':', \"'\", '+', '[', '\\\\', '@', '^', '{', '%', '(', '-', '\"', '*', '|', ',', '&', '<', '`', '}', '.', '_', '=', ']', '!', '>', ';', '?', '#', '$', ')', '/']\n",
        "punct_count=0\n",
        "for token in data.split():\n",
        "    if token in punctuation:\n",
        "        punct_count +=1\n",
        "print(\"Number of special characters:\", punct_count)\n",
        "\n",
        "numbers = []\n",
        "for word in data.split():\n",
        "    if word.isdigit():\n",
        "        numbers.append(int(word))\n",
        "print('Number of numerics :', len(numbers))\n",
        "\n",
        "upper_count = sum(map(str.isupper, data))\n",
        "print('Number of characters in uppercase :',upper_count)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PlaintextCorpusReader in '/content/sample_data'>\n",
            "Number of sentences = 177\n",
            "Number of words = 4476\n",
            "Number of characters = 16720\n",
            "Average word length: 4.510385756676558\n",
            "The number of stop words = 1679\n",
            "Number of special characters: 13\n",
            "Number of numerics : 60\n",
            "Number of characters in uppercase : 695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPa5O8RkLUgJ"
      },
      "source": [
        "## Text Pre-processing of text data\n",
        "#considering each sentence, individually\n",
        "\n",
        "import spacy\n",
        "import pysbd\n",
        "f = open(\"/content/sample_data/AdamsVsTanner.txt\", \"r\")\n",
        "data=f.read()\n",
        "seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
        "sentences=seg.segment(data)\n",
        "#sentences\n",
        "\n",
        "#change to lowercse\n",
        "#remove punctuation\n",
        "#remove stop words\n",
        "lower_case=[]\n",
        "for i in sentences:\n",
        "  lower_case.append(i.lower())      #converted to lower case letters\n",
        "uncleaned_df=pd.DataFrame(lower_case,columns=['Data'])    #removed all the punctuation\n",
        "df= uncleaned_df['Data'].apply(lambda x:''.join([i for i in x if i not in string.punctuation])).to_frame()\n",
        "df2= df['Data'].map(lambda x: re.sub(r'\\W+', ' ', x))\n",
        "cleaned_data=df2.values.tolist()\n",
        "#cleaned_data\n",
        "def remove(list):                   #Removed numbers from data and calculated words\n",
        "    pattern = '[0-9]'\n",
        "    list = [re.sub(pattern, '', i) for i in list] \n",
        "    return list\n",
        "clean_data=remove(cleaned_data)\n",
        "clean_data\n",
        "final_sent=[]\n",
        "for i in clean_data:\n",
        "  stop_words = set(stopwords.words('english'))        #Removed all the stop words,numbers\n",
        "  word_tokens = word_tokenize(i) \n",
        "  filtered_sentences = [w for w in word_tokens if not w in stop_words] \n",
        "  filtered_sentence = [] \n",
        "  for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        filtered_sentence.append(w)\n",
        "  final_sent.append(' '.join(word for word in filtered_sentence))\n",
        "#final_sent"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAkqahyVYYMn",
        "outputId": "310f7deb-69c8-46ee-b31f-aa9da2b79291",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "!pip install pyspellchecker"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspellchecker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/96/827c132397d0eb5731c1eda05dbfb019ede064ca8c7d0f329160ce0a4acd/pyspellchecker-0.5.5-py2.py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 3.7MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.5.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVzl41QwXhFq",
        "outputId": "d1d031fb-8dcc-4768-e926-052da6077f11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#code to remove\n",
        "from spellchecker import SpellChecker\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "spell = SpellChecker() \n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "rep_rem=[]\n",
        "for a in final_sent:                                    #Removed all the top 5 Repeated words and top 5 rare words\n",
        "  word_tokens = word_tokenize(a)                    #Tokenization\n",
        "  top_least_removed = [w for w in word_tokens if not w in top_least] \n",
        "  top_least_removed = [] \n",
        "  for w in word_tokens: \n",
        "    if w not in top_least: \n",
        "      spell_check=spell.correction(w)   #spelling check\n",
        "      stemmed=ps.stem(spell_check)        #stemming\n",
        "      lemmatized=lemmatizer.lemmatize(stemmed)   #Lemmatization     \n",
        "      top_least_removed.append(lemmatized)            \n",
        "  rep_rem.append(' '.join(word for word in top_least_removed))   \n",
        "#rep_rem\n",
        "while(\"\" in rep_rem) : \n",
        "    rep_rem.remove(\"\") \n",
        "rep_rem\n"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ala',\n",
              " 'court',\n",
              " 'june term',\n",
              " 'synopsi',\n",
              " 'writ error circuit court summer',\n",
              " 'west headnot',\n",
              " 'chattel mortgag',\n",
              " 'crop',\n",
              " 'grow exist subject-matt mortgag contract pas interest vest posse either immedi futur time',\n",
              " 'case cite headnot',\n",
              " 'creditor remedi',\n",
              " 'prioriti',\n",
              " 'st prohibit gather attach favor fi',\n",
              " 'fa',\n",
              " 'grow attach gather',\n",
              " 'case cite headnot',\n",
              " 'trial right properti statut',\n",
              " 'novemb issu circuit court summer suit plaintiff error requir sheriff counti make good c',\n",
              " 'allen harrison other sum thirty-seven hundr seventy-seven dollar besid cost',\n",
              " 'levi thirti bale cotton properti allen harrison claim bond given tri right',\n",
              " 'issu made tri question liabil cotton plaintiff submit juri',\n",
              " 'trial bill except seal instanc plaintiff',\n",
              " 'plaintiff prove recov judgment octob issu thereon th nov thereaft alia juri fieri facial issu regularli time made cotton levi growl plantat harrison cultiv hand servic',\n",
              " 'prove claimant product written contract harrison twenty-second may consider claimant involv inform burton harrison summer counti expo amount upward fourteen thousand dollar bargain sold claimant grow cotton c',\n",
              " 'consist one hundr twenti acr c',\n",
              " 'allen harrison promis oblig give use claimant time save suffer inform matur gather undertook deliv cotton janesvil',\n",
              " 'claimant came tennesse resid first septemb bring three four white labour took posse slave latter white labour gather cotton prepar market levi warehous janesvil',\n",
              " 'plaintiff prove harrison claimant took posse absent dispos without consent',\n",
              " 'admit contract made good faith',\n",
              " 'court charg juri plaintiff virtu judgment grow harrison right convey without manner restrain write adduc sale fieri facial would attach upon gather yet claimant obtain posse first septemb control gather attach subject',\n",
              " 'attorney law firm',\n",
              " 'r h smith plaintiff error made follow point',\n",
              " 'harrison must may immatur state insist subject sale',\n",
              " 'common law grow could levi sold talk rep bo p rep east rep note john rep mass rep statu ask dig p forbid grow receiv strict construct',\n",
              " 'mere inhibit attach sale may made matur gather',\n",
              " 'contract purport convey grow mere executori agreement requir act done harrison order invest claimant right properti',\n",
              " 'chit con john rep wend rep john rep down rep act done matter gather liabl seiz harrison debt',\n",
              " 'court chanceri would compel specif perform contract claimant instanc',\n",
              " 'charg court also objection decid disput fact',\n",
              " 'w murphi w g jone defendantcit act ask dig declar law plant gather contend attach favor plaintiff',\n",
              " 'case defend restrain make contract claimant',\n",
              " 'destroy injunct take away right',\n",
              " 'short whenev right temporarili suspend withdrawn time lost',\n",
              " 'whippl foot john rep wash c c rep rep',\n",
              " 'admit contract defend claimant good faith sever remov cotton gave latter good titl creditor former',\n",
              " 'opinion',\n",
              " 'collier c j',\n",
              " 'doubt grow exist subject matter sale mortgag contract posse interest vest posse either immedi futur time',\n",
              " 'proposit frequent assum unquestion point inquiri gener whether statut fraud cha',\n",
              " 'mere chattel transfer parol without write',\n",
              " 'chitti con whippl foot john rep stewart doughti john rep austin sawyer cow rep see also rames lee alston last term contract set bill except inclin think evid rather mortgag absolut sale',\n",
              " 'recit claimant involv inform mercantil firm defend partner upward fourteen thousand dollar estat sheriff hand convey made cotton corn oat grantor agre give time use claimant prevent injuri inform',\n",
              " 'defend might time divest interest contract vest claimant discharg liabil inform judgment creditor might satisfi gather levi sold fieri facial',\n",
              " 'consid write claimant assert right mortgag power take posse time year unless reliev engag inform',\n",
              " 'pretend liabil satisfi admit parti act good faith dri question law whether right plaintiff claimant shall prevail',\n",
              " 'assum present plaintiff oper upon plant previou contract may inquir whether defend interest could levi sold',\n",
              " 'claimant previou taken posse prepar cotton market remov warehous',\n",
              " 'posse insist trespass acquir absenc defend without consent given',\n",
              " 'conced truth fact state bill except think follow posse claimant nulliti case must consid never interf',\n",
              " 'contract contain express undertak give time claimant might requir indemn took posse absenc grantor though without consent subsequ acquiesc infer would necessari act approv',\n",
              " 'take clear law seen defend time noth mere equit right redeem cotton pay debt endors claimant',\n",
              " 'posse coupl equiti nake equiti held reach ordinari',\n",
              " 'perkin elliott mayfield porter rep',\n",
              " 'bring u back question whether plaintiff grow defeat mortgag claimant',\n",
              " 'frequent moot whether common law corn c',\n",
              " 'gather seiz fieri facial',\n",
              " 'mr dane remark upon point say american editor bacon abridg say wheat grow ground chattel subject taken sheriff may suffer grow till harvest cut sell may perhap sell grow purchas entitl enter purpos cut carri away cite whippl foot ut supra also pool case talk bo p east n whippl foot seem case support posit unrip wheat corn may taken editor state noth taken sold',\n",
              " 'posit say learn comment doubt law',\n",
              " 'unnecessari consid matter stand common law',\n",
              " 'first section act prevent sheriff offic levi execut certain case enact shall law sheriff offic writ fieri facial plant debtor person may issu gather ask dig express inhibit remain ground sever soil owe growth',\n",
              " 'respect properti thu situat attach eo instant upon place hand offic',\n",
              " 'act cite effect keep right abey gather',\n",
              " 'oper prevent debtor dispos properti attach give creditor right sold satisfi judgment',\n",
              " 'right intim connect latter taken away suspend effect common law destruct former',\n",
              " 'principl fulli establish manson harwel presid c',\n",
              " 'bank unit state assigne citat contain opinion court case also opinion wood gari et al decid last term',\n",
              " 'compet legislatur made unlaw particular properti condit chang still give continu doubt noth act question indic intent',\n",
              " 'object mere suspend sale gather would easi said explicit term declar statut totem verb shall levi legislatur must suppos meant express',\n",
              " 'act induc doubt exist common law intend remov doubt declar law futur',\n",
              " 'creat author case law exist silent',\n",
              " 'idea attach upon plant soon deliv sheriff though right postpon sever took place attempt deduc last word section cite viz gather',\n",
              " 'word upon principl construct regard potent give retrospect effect',\n",
              " 'refer would postpon gather relat postpon event take place',\n",
              " 'right plant expressli taken away statut connect consequ upon right never attach sever',\n",
              " 'case right defend make contract unquestion titl claimant coupl posse paramount could exert',\n",
              " 'circuit judg may mistaken law suppos contract sale error respect immateri whether sale mortgag seen fact case defend interest could seiz sold',\n",
              " 'assumpt materi fact charg posse claimant time acquir gather c',\n",
              " 'refer determin juri instruct find accord evid adduc ever attach favor plaintiff',\n",
              " 'bona fide contract conced charg necessari point could proprieti enter inquiri juri',\n",
              " 'result said judgment circuit court affirm',\n",
              " 'dissent opinion',\n",
              " 'ormond j',\n",
              " 'statut present question court shall law sheriff offic writ fidei facial plant debtor person may issu gather clay dig',\n",
              " 'shall enter upon enquiri whether common law could levi upon grow though apprehend would difficult maintain affirm proposit',\n",
              " 'suffici purpos statut suppos law doubtless practic',\n",
              " 'act must consid connect act upon subject',\n",
              " 'polici state indic statut undeni properti debtor real person legal titl shall subject sale appear would difficult assign reason exempt speci properti claim judgment creditor give defend right dispos',\n",
              " 'appear defer argument sheriff prohibit levi plant therefor lost debtor right sell non loquitur',\n",
              " 'mischief statut design remedi sacrific would necessarili made sale immatur statut enabl debtor retain matur sever soil put condit bring valueth mean time continu plaintiff',\n",
              " 'confirm correct view necessari found think languag employ legislatur',\n",
              " 'sheriff forbidden plant gather',\n",
              " 'view taken major court correct right secur plaintiff levi gather may frustrat case sale defend whilst immatur state',\n",
              " 'construct put upon statut involv singular anomali legislatur protect debtor forbidden plaintiff sell properti debtor condit bring valu yet permit debtor voluntarili sale submit sacrific benefit',\n",
              " 'effect gift defend grow provid gather dispos condit',\n",
              " 'feel thorough convict intent legislatur secur loss prohibit sale gather temporari suspens right sell ceas',\n",
              " 'citat',\n",
              " 'ala we',\n",
              " 'end document',\n",
              " 'thomson reuter',\n",
              " 'claim origin u govern work',\n",
              " 'cite refer',\n",
              " 'treatment',\n",
              " 'titl',\n",
              " 'date',\n",
              " 'type',\n",
              " 'depth',\n",
              " 'headnot',\n",
              " 'cite',\n",
              " 'booker jone am x',\n",
              " 'ala ala',\n",
              " 'trover convers cotton count case',\n",
              " 'appeal circuit court hale',\n",
              " 'tri hon',\n",
              " 'j scaffold',\n",
              " 'dec term',\n",
              " 'case',\n",
              " 'cite',\n",
              " 'lehman burr co marshal',\n",
              " 'ala ala',\n",
              " 'trover convers cotton appeal citi court montgomeri',\n",
              " 'tri hon',\n",
              " 'john cunningham',\n",
              " 'jan term',\n",
              " 'case',\n",
              " 'cite',\n",
              " 'bib jancey',\n",
              " 'ala ala',\n",
              " 'banish wage waiver exempt appeal citi court montgomeri',\n",
              " 'tri hon',\n",
              " 'john cunningham',\n",
              " 'jan term',\n",
              " 'case',\n",
              " 'cite',\n",
              " 'mckenzi lamprey',\n",
              " 'ala ala',\n",
              " 'trial right properti cotton appeal circuit court barbour',\n",
              " 'tri hon',\n",
              " 'hale',\n",
              " 'jan term',\n",
              " 'case',\n",
              " 'cite',\n",
              " 'evan lamar',\n",
              " 'ala ala',\n",
              " 'error circuit court autauga',\n",
              " 'tri hon',\n",
              " 'b moor',\n",
              " 'jun term',\n",
              " 'case',\n",
              " 'cite',\n",
              " 'dewey bowman',\n",
              " 'cal cal',\n",
              " 'judgment court jacob cohen revers follow reason find court far cohen concern',\n",
              " 'jul term',\n",
              " 'case',\n",
              " 'mention',\n",
              " 'ree coat',\n",
              " 'ala ala',\n",
              " 'trover convers three bale cotton',\n",
              " 'appeal circuit court ecowa',\n",
              " 'tri hon',\n",
              " 'wm',\n",
              " 'l whitlock',\n",
              " 'nov term',\n",
              " 'case',\n",
              " 'mention',\n",
              " 'edward thompson',\n",
              " 'sw ten',\n",
              " 'appeal circuit court weakli counti',\n",
              " 'may',\n",
              " 'case',\n",
              " 'grow crop subject seizur attach',\n",
              " 'all',\n",
              " 'gener common law grow crop rais annual plant still attach soil regard person chattel subject seizur attach',\n",
              " 'all',\n",
              " 'tabl author',\n",
              " 'treatment',\n",
              " 'referenc titl',\n",
              " 'type',\n",
              " 'depth',\n",
              " 'quot',\n",
              " 'page number',\n",
              " 'mention',\n",
              " 'austin sawyer',\n",
              " 'cow',\n",
              " 'syrup',\n",
              " 'parol evid admiss contradict substanti vari written contract',\n",
              " 'quitclaim land w wheat grow reserv',\n",
              " 'case',\n",
              " 'cite',\n",
              " 'perkin mayfield',\n",
              " 'port',\n",
              " 'ala',\n",
              " 'writ error circuit court tuskaloosa',\n",
              " 'case',\n",
              " 'mention',\n",
              " 'stewart doughti',\n",
              " 'john',\n",
              " 'syrup',\n",
              " 'let b farm six year agre render yield pay one half wheat rye corn grain rais farm year',\n",
              " 'case',\n",
              " 'file',\n",
              " 'file citat',\n",
              " 'neg treatment',\n",
              " 'neg treatment result citat',\n",
              " 'histori',\n",
              " 'histori result citat']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mHlyWlqTnSl"
      },
      "source": [
        "# 1.3 Save all the clean sentences to a csv file (one column, each raw is a sentence) after finishing all the steps above.\n",
        "pd.DataFrame(rep_rem,columns=['cleaned']).to_csv('final.csv')\n"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhhlbWODTpgk",
        "outputId": "8ce87690-491f-486c-a5c9-63beb3b29187",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#1.4 Advance Text Processing (Extra credit: 4 points)\n",
        "#Calculate the term frequency of all the terms.\n",
        "def freq(str): \n",
        "\tstr_list = str.split() \n",
        "\tunique_words = set(str_list) \t    #Frequency of all the terms\n",
        "\tfor o in unique_words : \n",
        "\t\tprint('Frequency Of-->', o , '->', str_list.count(o)) \n",
        "st=''\n",
        "for k in words:\n",
        "  st+=' '+k\n",
        "freq(st)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Frequency Of--> were -> 4\n",
            "Frequency Of--> facias -> 6\n",
            "Frequency Of--> first -> 3\n",
            "Frequency Of--> remedy -> 1\n",
            "Frequency Of--> until -> 18\n",
            "Frequency Of--> prohibiting -> 2\n",
            "Frequency Of--> rep -> 16\n",
            "Frequency Of--> purchaser -> 1\n",
            "Frequency Of--> reuters -> 1\n",
            "Frequency Of--> right -> 21\n",
            "Frequency Of--> possession -> 13\n",
            "Frequency Of--> us -> 2\n",
            "Frequency Of--> growed -> 1\n",
            "Frequency Of--> operate -> 2\n",
            "Frequency Of--> disposed -> 1\n",
            "Frequency Of--> redeem -> 1\n",
            "Frequency Of--> objectionable -> 1\n",
            "Frequency Of--> law -> 15\n",
            "Frequency Of--> suspended -> 2\n",
            "Frequency Of--> jan -> 3\n",
            "Frequency Of--> bill -> 3\n",
            "Frequency Of--> thus -> 1\n",
            "Frequency Of--> white -> 2\n",
            "Frequency Of--> planted -> 8\n",
            "Frequency Of--> divested -> 1\n",
            "Frequency Of--> if -> 16\n",
            "Frequency Of--> sufficient -> 1\n",
            "Frequency Of--> assignees -> 1\n",
            "Frequency Of--> valuethe -> 1\n",
            "Frequency Of--> title -> 5\n",
            "Frequency Of--> remove -> 1\n",
            "Frequency Of--> facts -> 3\n",
            "Frequency Of--> thereafter -> 1\n",
            "Frequency Of--> supposing -> 1\n",
            "Frequency Of--> argument -> 1\n",
            "Frequency Of--> established -> 1\n",
            "Frequency Of--> receive -> 1\n",
            "Frequency Of--> do -> 1\n",
            "Frequency Of--> we -> 5\n",
            "Frequency Of--> his -> 14\n",
            "Frequency Of--> v -> 22\n",
            "Frequency Of--> refer -> 1\n",
            "Frequency Of--> generally -> 2\n",
            "Frequency Of--> fourteen -> 2\n",
            "Frequency Of--> follow -> 1\n",
            "Frequency Of--> former -> 2\n",
            "Frequency Of--> whitlock -> 1\n",
            "Frequency Of--> frauds -> 1\n",
            "Frequency Of--> species -> 1\n",
            "Frequency Of--> latter -> 3\n",
            "Frequency Of--> view -> 2\n",
            "Frequency Of--> concerned -> 1\n",
            "Frequency Of--> note -> 1\n",
            "Frequency Of--> obtained -> 1\n",
            "Frequency Of--> court -> 20\n",
            "Frequency Of--> said -> 2\n",
            "Frequency Of--> tennessee -> 1\n",
            "Frequency Of--> affirmed -> 1\n",
            "Frequency Of--> what -> 4\n",
            "Frequency Of--> instance -> 2\n",
            "Frequency Of--> r -> 1\n",
            "Frequency Of--> tanner -> 1\n",
            "Frequency Of--> consideration -> 1\n",
            "Frequency Of--> order -> 1\n",
            "Frequency Of--> took -> 4\n",
            "Frequency Of--> very -> 2\n",
            "Frequency Of--> but -> 15\n",
            "Frequency Of--> discharging -> 1\n",
            "Frequency Of--> evans -> 1\n",
            "Frequency Of--> specific -> 1\n",
            "Frequency Of--> claimant -> 2\n",
            "Frequency Of--> attempted -> 1\n",
            "Frequency Of--> non -> 1\n",
            "Frequency Of--> alabama -> 1\n",
            "Frequency Of--> suffer -> 1\n",
            "Frequency Of--> placed -> 1\n",
            "Frequency Of--> editor -> 2\n",
            "Frequency Of--> apprehend -> 1\n",
            "Frequency Of--> resided -> 1\n",
            "Frequency Of--> legislature -> 5\n",
            "Frequency Of--> voluntarily -> 1\n",
            "Frequency Of--> dane -> 1\n",
            "Frequency Of--> half -> 1\n",
            "Frequency Of--> did -> 5\n",
            "Frequency Of--> those -> 1\n",
            "Frequency Of--> removed -> 1\n",
            "Frequency Of--> whether -> 7\n",
            "Frequency Of--> growth -> 1\n",
            "Frequency Of--> creditors -> 3\n",
            "Frequency Of--> present -> 1\n",
            "Frequency Of--> practice -> 1\n",
            "Frequency Of--> either -> 2\n",
            "Frequency Of--> porter -> 1\n",
            "Frequency Of--> person -> 2\n",
            "Frequency Of--> janney -> 1\n",
            "Frequency Of--> against -> 5\n",
            "Frequency Of--> evidences -> 1\n",
            "Frequency Of--> convey -> 2\n",
            "Frequency Of--> perkins -> 2\n",
            "Frequency Of--> dissenting -> 1\n",
            "Frequency Of--> benefit -> 1\n",
            "Frequency Of--> headnotes -> 2\n",
            "Frequency Of--> deduced -> 1\n",
            "Frequency Of--> montgomery -> 2\n",
            "Frequency Of--> power -> 1\n",
            "Frequency Of--> undeniably -> 1\n",
            "Frequency Of--> which -> 25\n",
            "Frequency Of--> lost -> 2\n",
            "Frequency Of--> suit -> 1\n",
            "Frequency Of--> ut -> 1\n",
            "Frequency Of--> mckenzie -> 1\n",
            "Frequency Of--> forbidden -> 2\n",
            "Frequency Of--> strict -> 1\n",
            "Frequency Of--> correctness -> 1\n",
            "Frequency Of--> recites -> 1\n",
            "Frequency Of--> induced -> 1\n",
            "Frequency Of--> thereon -> 1\n",
            "Frequency Of--> severed -> 1\n",
            "Frequency Of--> secure -> 1\n",
            "Frequency Of--> ground -> 2\n",
            "Frequency Of--> writ -> 4\n",
            "Frequency Of--> time -> 12\n",
            "Frequency Of--> make -> 2\n",
            "Frequency Of--> than -> 2\n",
            "Frequency Of--> remains -> 1\n",
            "Frequency Of--> lee -> 1\n",
            "Frequency Of--> amounting -> 1\n",
            "Frequency Of--> debtor -> 9\n",
            "Frequency Of--> table -> 1\n",
            "Frequency Of--> sw -> 1\n",
            "Frequency Of--> he -> 11\n",
            "Frequency Of--> history -> 2\n",
            "Frequency Of--> permits -> 1\n",
            "Frequency Of--> perhaps -> 1\n",
            "Frequency Of--> chattel -> 3\n",
            "Frequency Of--> al -> 1\n",
            "Frequency Of--> citations -> 2\n",
            "Frequency Of--> back -> 1\n",
            "Frequency Of--> states -> 2\n",
            "Frequency Of--> that -> 56\n",
            "Frequency Of--> so -> 10\n",
            "Frequency Of--> presents -> 1\n",
            "Frequency Of--> hon -> 6\n",
            "Frequency Of--> reason -> 1\n",
            "Frequency Of--> mooted -> 1\n",
            "Frequency Of--> executions -> 1\n",
            "Frequency Of--> doubt -> 2\n",
            "Frequency Of--> attorneys -> 1\n",
            "Frequency Of--> doubtless -> 1\n",
            "Frequency Of--> farm -> 2\n",
            "Frequency Of--> crop -> 49\n",
            "Frequency Of--> tenn -> 1\n",
            "Frequency Of--> correct -> 1\n",
            "Frequency Of--> year -> 2\n",
            "Frequency Of--> jacob -> 1\n",
            "Frequency Of--> performance -> 1\n",
            "Frequency Of--> hundred -> 2\n",
            "Frequency Of--> attaches -> 4\n",
            "Frequency Of--> grow -> 1\n",
            "Frequency Of--> changed -> 1\n",
            "Frequency Of--> each -> 1\n",
            "Frequency Of--> inhibition -> 1\n",
            "Frequency Of--> promised -> 1\n",
            "Frequency Of--> three -> 2\n",
            "Frequency Of--> co -> 1\n",
            "Frequency Of--> finding -> 1\n",
            "Frequency Of--> suffering -> 1\n",
            "Frequency Of--> smith -> 1\n",
            "Frequency Of--> propriety -> 1\n",
            "Frequency Of--> case -> 21\n",
            "Frequency Of--> all -> 7\n",
            "Frequency Of--> doughty -> 2\n",
            "Frequency Of--> considered -> 2\n",
            "Frequency Of--> give -> 5\n",
            "Frequency Of--> firm -> 1\n",
            "Frequency Of--> six -> 1\n",
            "Frequency Of--> the -> 337\n",
            "Frequency Of--> conviction -> 1\n",
            "Frequency Of--> existed -> 2\n",
            "Frequency Of--> therefore -> 1\n",
            "Frequency Of--> type -> 2\n",
            "Frequency Of--> following -> 2\n",
            "Frequency Of--> judge -> 1\n",
            "Frequency Of--> attached -> 6\n",
            "Frequency Of--> intended -> 1\n",
            "Frequency Of--> allen -> 3\n",
            "Frequency Of--> proved -> 3\n",
            "Frequency Of--> warehouse -> 2\n",
            "Frequency Of--> charged -> 1\n",
            "Frequency Of--> gave -> 1\n",
            "Frequency Of--> port -> 1\n",
            "Frequency Of--> rye -> 1\n",
            "Frequency Of--> matures -> 2\n",
            "Frequency Of--> see -> 1\n",
            "Frequency Of--> absolute -> 1\n",
            "Frequency Of--> p -> 3\n",
            "Frequency Of--> temporary -> 1\n",
            "Frequency Of--> himself -> 2\n",
            "Frequency Of--> filings -> 2\n",
            "Frequency Of--> ordinary -> 1\n",
            "Frequency Of--> necessary -> 3\n",
            "Frequency Of--> wood -> 1\n",
            "Frequency Of--> market -> 2\n",
            "Frequency Of--> respect -> 2\n",
            "Frequency Of--> language -> 1\n",
            "Frequency Of--> remarking -> 1\n",
            "Frequency Of--> they -> 10\n",
            "Frequency Of--> mr -> 1\n",
            "Frequency Of--> previous -> 2\n",
            "Frequency Of--> instanti -> 1\n",
            "Frequency Of--> and -> 83\n",
            "Frequency Of--> meant -> 1\n",
            "Frequency Of--> sumter -> 3\n",
            "Frequency Of--> protection -> 1\n",
            "Frequency Of--> making -> 1\n",
            "Frequency Of--> inhibits -> 1\n",
            "Frequency Of--> there -> 6\n",
            "Frequency Of--> corn -> 4\n",
            "Frequency Of--> disposing -> 1\n",
            "Frequency Of--> dispose -> 1\n",
            "Frequency Of--> involves -> 1\n",
            "Frequency Of--> bargained -> 1\n",
            "Frequency Of--> gainesville -> 2\n",
            "Frequency Of--> etowah -> 1\n",
            "Frequency Of--> assumed -> 1\n",
            "Frequency Of--> nysup -> 2\n",
            "Frequency Of--> appears -> 2\n",
            "Frequency Of--> far -> 1\n",
            "Frequency Of--> common -> 7\n",
            "Frequency Of--> one -> 2\n",
            "Frequency Of--> does -> 6\n",
            "Frequency Of--> officers -> 1\n",
            "Frequency Of--> headnote -> 2\n",
            "Frequency Of--> levying -> 3\n",
            "Frequency Of--> eo -> 1\n",
            "Frequency Of--> conversion -> 3\n",
            "Frequency Of--> autauga -> 1\n",
            "Frequency Of--> foot -> 4\n",
            "Frequency Of--> number -> 1\n",
            "Frequency Of--> negative -> 2\n",
            "Frequency Of--> restrained -> 2\n",
            "Frequency Of--> mentioned -> 4\n",
            "Frequency Of--> ala -> 15\n",
            "Frequency Of--> yield -> 1\n",
            "Frequency Of--> mortgages -> 1\n",
            "Frequency Of--> whenever -> 1\n",
            "Frequency Of--> cal -> 2\n",
            "Frequency Of--> sheriffs -> 1\n",
            "Frequency Of--> obliged -> 1\n",
            "Frequency Of--> to -> 116\n",
            "Frequency Of--> about -> 1\n",
            "Frequency Of--> agreement -> 1\n",
            "Frequency Of--> construction -> 3\n",
            "Frequency Of--> immediately -> 2\n",
            "Frequency Of--> chitty -> 1\n",
            "Frequency Of--> assuming -> 1\n",
            "Frequency Of--> verbis -> 1\n",
            "Frequency Of--> dry -> 1\n",
            "Frequency Of--> invest -> 1\n",
            "Frequency Of--> bibb -> 1\n",
            "Frequency Of--> cutting -> 1\n",
            "Frequency Of--> exposed -> 1\n",
            "Frequency Of--> depth -> 2\n",
            "Frequency Of--> found -> 1\n",
            "Frequency Of--> point -> 3\n",
            "Frequency Of--> attach -> 2\n",
            "Frequency Of--> inquire -> 1\n",
            "Frequency Of--> abridgment -> 1\n",
            "Frequency Of--> frequently -> 2\n",
            "Frequency Of--> mortgage -> 6\n",
            "Frequency Of--> seems -> 1\n",
            "Frequency Of--> effect -> 4\n",
            "Frequency Of--> nor -> 1\n",
            "Frequency Of--> bona -> 1\n",
            "Frequency Of--> regularly -> 1\n",
            "Frequency Of--> explicit -> 1\n",
            "Frequency Of--> conceded -> 1\n",
            "Frequency Of--> or -> 22\n",
            "Frequency Of--> just -> 1\n",
            "Frequency Of--> seized -> 2\n",
            "Frequency Of--> indemnity -> 1\n",
            "Frequency Of--> existence -> 2\n",
            "Frequency Of--> circuit -> 10\n",
            "Frequency Of--> sequitur -> 1\n",
            "Frequency Of--> would -> 8\n",
            "Frequency Of--> how -> 2\n",
            "Frequency Of--> difficult -> 2\n",
            "Frequency Of--> involved -> 2\n",
            "Frequency Of--> et -> 1\n",
            "Frequency Of--> pay -> 1\n",
            "Frequency Of--> submitted -> 1\n",
            "Frequency Of--> sale -> 13\n",
            "Frequency Of--> a -> 84\n",
            "Frequency Of--> j -> 3\n",
            "Frequency Of--> mean -> 1\n",
            "Frequency Of--> jun -> 1\n",
            "Frequency Of--> undertaking -> 1\n",
            "Frequency Of--> coats -> 1\n",
            "Frequency Of--> from -> 20\n",
            "Frequency Of--> cite -> 2\n",
            "Frequency Of--> disposes -> 1\n",
            "Frequency Of--> consisting -> 1\n",
            "Frequency Of--> gathering -> 2\n",
            "Frequency Of--> been -> 11\n",
            "Frequency Of--> given -> 2\n",
            "Frequency Of--> made -> 8\n",
            "Frequency Of--> without -> 5\n",
            "Frequency Of--> adduced -> 2\n",
            "Frequency Of--> terms -> 1\n",
            "Frequency Of--> enquiry -> 1\n",
            "Frequency Of--> inference -> 1\n",
            "Frequency Of--> bank -> 1\n",
            "Frequency Of--> entitled -> 1\n",
            "Frequency Of--> also -> 4\n",
            "Frequency Of--> city -> 2\n",
            "Frequency Of--> issued -> 3\n",
            "Frequency Of--> shall -> 6\n",
            "Frequency Of--> declares -> 1\n",
            "Frequency Of--> section -> 2\n",
            "Frequency Of--> unnecessary -> 1\n",
            "Frequency Of--> crops -> 3\n",
            "Frequency Of--> bos -> 2\n",
            "Frequency Of--> retrospective -> 1\n",
            "Frequency Of--> takes -> 2\n",
            "Frequency Of--> issue -> 3\n",
            "Frequency Of--> besides -> 1\n",
            "Frequency Of--> could -> 6\n",
            "Frequency Of--> being -> 6\n",
            "Frequency Of--> take -> 1\n",
            "Frequency Of--> declaring -> 2\n",
            "Frequency Of--> virtue -> 1\n",
            "Frequency Of--> favor -> 3\n",
            "Frequency Of--> try -> 2\n",
            "Frequency Of--> ever -> 1\n",
            "Frequency Of--> up -> 5\n",
            "Frequency Of--> agreed -> 1\n",
            "Frequency Of--> had -> 9\n",
            "Frequency Of--> dig -> 4\n",
            "Frequency Of--> whipple -> 4\n",
            "Frequency Of--> still -> 2\n",
            "Frequency Of--> material -> 1\n",
            "Frequency Of--> any -> 11\n",
            "Frequency Of--> relieved -> 1\n",
            "Frequency Of--> durr -> 1\n",
            "Frequency Of--> statute -> 9\n",
            "Frequency Of--> cunningham -> 2\n",
            "Frequency Of--> austin -> 2\n",
            "Frequency Of--> pretended -> 1\n",
            "Frequency Of--> unquestionable -> 2\n",
            "Frequency Of--> only -> 4\n",
            "Frequency Of--> laborers -> 2\n",
            "Frequency Of--> have -> 16\n",
            "Frequency Of--> executory -> 1\n",
            "Frequency Of--> idea -> 1\n",
            "Frequency Of--> this -> 24\n",
            "Frequency Of--> consent -> 3\n",
            "Frequency Of--> condition -> 4\n",
            "Frequency Of--> claims -> 1\n",
            "Frequency Of--> quoted -> 1\n",
            "Frequency Of--> bringing -> 1\n",
            "Frequency Of--> faith -> 3\n",
            "Frequency Of--> principles -> 1\n",
            "Frequency Of--> substantially -> 1\n",
            "Frequency Of--> error -> 6\n",
            "Frequency Of--> between -> 1\n",
            "Frequency Of--> works -> 1\n",
            "Frequency Of--> attachment -> 2\n",
            "Frequency Of--> though -> 3\n",
            "Frequency Of--> reversed -> 1\n",
            "Frequency Of--> rather -> 1\n",
            "Frequency Of--> chancery -> 1\n",
            "Frequency Of--> thousand -> 2\n",
            "Frequency Of--> fi -> 1\n",
            "Frequency Of--> debts -> 2\n",
            "Frequency Of--> sacrifice -> 2\n",
            "Frequency Of--> dollars -> 3\n",
            "Frequency Of--> express -> 2\n",
            "Frequency Of--> injury -> 1\n",
            "Frequency Of--> deciding -> 1\n",
            "Frequency Of--> insisted -> 2\n",
            "Frequency Of--> injunction -> 1\n",
            "Frequency Of--> more -> 1\n",
            "Frequency Of--> place -> 2\n",
            "Frequency Of--> adams -> 1\n",
            "Frequency Of--> tuskaloosa -> 1\n",
            "Frequency Of--> not -> 22\n",
            "Frequency Of--> designed -> 1\n",
            "Frequency Of--> referenced -> 1\n",
            "Frequency Of--> collier -> 1\n",
            "Frequency Of--> immature -> 3\n",
            "Frequency Of--> interfered -> 1\n",
            "Frequency Of--> same -> 3\n",
            "Frequency Of--> enter -> 3\n",
            "Frequency Of--> cited -> 9\n",
            "Frequency Of--> now -> 2\n",
            "Frequency Of--> thomson -> 1\n",
            "Frequency Of--> expressly -> 1\n",
            "Frequency Of--> admissible -> 1\n",
            "Frequency Of--> an -> 33\n",
            "Frequency Of--> gather -> 1\n",
            "Frequency Of--> compel -> 1\n",
            "Frequency Of--> soon -> 1\n",
            "Frequency Of--> poole -> 1\n",
            "Frequency Of--> taken -> 7\n",
            "Frequency Of--> as -> 32\n",
            "Frequency Of--> retain -> 1\n",
            "Frequency Of--> x -> 1\n",
            "Frequency Of--> object -> 1\n",
            "Frequency Of--> whom -> 3\n",
            "Frequency Of--> con -> 2\n",
            "Frequency Of--> dowl -> 1\n",
            "Frequency Of--> acted -> 1\n",
            "Frequency Of--> event -> 1\n",
            "Frequency Of--> clay -> 1\n",
            "Frequency Of--> barbour -> 1\n",
            "Frequency Of--> agrees -> 1\n",
            "Frequency Of--> me -> 2\n",
            "Frequency Of--> fully -> 1\n",
            "Frequency Of--> hurtell -> 1\n",
            "Frequency Of--> postponed -> 1\n",
            "Frequency Of--> intimately -> 1\n",
            "Frequency Of--> acquiesced -> 1\n",
            "Frequency Of--> statutes -> 1\n",
            "Frequency Of--> edwards -> 1\n",
            "Frequency Of--> let -> 1\n",
            "Frequency Of--> unless -> 1\n",
            "Frequency Of--> according -> 1\n",
            "Frequency Of--> cultivated -> 1\n",
            "Frequency Of--> during -> 2\n",
            "Frequency Of--> costs -> 1\n",
            "Frequency Of--> jul -> 1\n",
            "Frequency Of--> unripe -> 1\n",
            "Frequency Of--> then -> 10\n",
            "Frequency Of--> lehman -> 1\n",
            "Frequency Of--> determination -> 1\n",
            "Frequency Of--> points -> 1\n",
            "Frequency Of--> admitted -> 3\n",
            "Frequency Of--> engagements -> 1\n",
            "Frequency Of--> soil -> 3\n",
            "Frequency Of--> totidem -> 1\n",
            "Frequency Of--> these -> 2\n",
            "Frequency Of--> necessarily -> 1\n",
            "Frequency Of--> regarded -> 2\n",
            "Frequency Of--> citing -> 1\n",
            "Frequency Of--> state -> 3\n",
            "Frequency Of--> under -> 10\n",
            "Frequency Of--> seen -> 2\n",
            "Frequency Of--> words -> 2\n",
            "Frequency Of--> wm -> 1\n",
            "Frequency Of--> done -> 2\n",
            "Frequency Of--> thompson -> 1\n",
            "Frequency Of--> jury -> 4\n",
            "Frequency Of--> s -> 12\n",
            "Frequency Of--> property -> 10\n",
            "Frequency Of--> brings -> 1\n",
            "Frequency Of--> suspension -> 1\n",
            "Frequency Of--> levied -> 8\n",
            "Frequency Of--> evidence -> 2\n",
            "Frequency Of--> claimed -> 1\n",
            "Frequency Of--> easy -> 1\n",
            "Frequency Of--> value -> 1\n",
            "Frequency Of--> claimants -> 21\n",
            "Frequency Of--> fides -> 1\n",
            "Frequency Of--> provided -> 1\n",
            "Frequency Of--> priority -> 1\n",
            "Frequency Of--> planting -> 1\n",
            "Frequency Of--> legal -> 1\n",
            "Frequency Of--> american -> 1\n",
            "Frequency Of--> seised -> 1\n",
            "Frequency Of--> mass -> 1\n",
            "Frequency Of--> chit -> 1\n",
            "Frequency Of--> reserving -> 1\n",
            "Frequency Of--> contained -> 1\n",
            "Frequency Of--> submit -> 1\n",
            "Frequency Of--> purport -> 1\n",
            "Frequency Of--> equitable -> 1\n",
            "Frequency Of--> absence -> 2\n",
            "Frequency Of--> others -> 1\n",
            "Frequency Of--> prepared -> 2\n",
            "Frequency Of--> h -> 1\n",
            "Frequency Of--> th -> 1\n",
            "Frequency Of--> mischief -> 1\n",
            "Frequency Of--> abeyance -> 1\n",
            "Frequency Of--> supra -> 1\n",
            "Frequency Of--> moore -> 1\n",
            "Frequency Of--> withdrawn -> 1\n",
            "Frequency Of--> transferrable -> 1\n",
            "Frequency Of--> render -> 1\n",
            "Frequency Of--> slaves -> 1\n",
            "Frequency Of--> intention -> 2\n",
            "Frequency Of--> in -> 81\n",
            "Frequency Of--> came -> 1\n",
            "Frequency Of--> gary -> 1\n",
            "Frequency Of--> execution -> 50\n",
            "Frequency Of--> act -> 8\n",
            "Frequency Of--> commentator -> 1\n",
            "Frequency Of--> paramount -> 1\n",
            "Frequency Of--> inquiry -> 2\n",
            "Frequency Of--> ceased -> 1\n",
            "Frequency Of--> goods -> 1\n",
            "Frequency Of--> date -> 1\n",
            "Frequency Of--> connection -> 1\n",
            "Frequency Of--> vest -> 2\n",
            "Frequency Of--> fieri -> 5\n",
            "Frequency Of--> east -> 2\n",
            "Frequency Of--> alr -> 2\n",
            "Frequency Of--> never -> 2\n",
            "Frequency Of--> land -> 1\n",
            "Frequency Of--> stewart -> 2\n",
            "Frequency Of--> west -> 1\n",
            "Frequency Of--> contains -> 1\n",
            "Frequency Of--> enables -> 1\n",
            "Frequency Of--> removal -> 1\n",
            "Frequency Of--> president -> 1\n",
            "Frequency Of--> affirmative -> 1\n",
            "Frequency Of--> exert -> 1\n",
            "Frequency Of--> interest -> 5\n",
            "Frequency Of--> fieei -> 1\n",
            "Frequency Of--> them -> 5\n",
            "Frequency Of--> chas -> 1\n",
            "Frequency Of--> anomaly -> 1\n",
            "Frequency Of--> must -> 4\n",
            "Frequency Of--> county -> 3\n",
            "Frequency Of--> w -> 3\n",
            "Frequency Of--> position -> 2\n",
            "Frequency Of--> situated -> 1\n",
            "Frequency Of--> estate -> 1\n",
            "Frequency Of--> by -> 39\n",
            "Frequency Of--> carrying -> 1\n",
            "Frequency Of--> plaintiffs -> 1\n",
            "Frequency Of--> l -> 1\n",
            "Frequency Of--> use -> 2\n",
            "Frequency Of--> truth -> 1\n",
            "Frequency Of--> june -> 1\n",
            "Frequency Of--> stands -> 1\n",
            "Frequency Of--> prevent -> 3\n",
            "Frequency Of--> term -> 10\n",
            "Frequency Of--> passes -> 1\n",
            "Frequency Of--> reached -> 1\n",
            "Frequency Of--> wend -> 1\n",
            "Frequency Of--> assert -> 1\n",
            "Frequency Of--> may -> 12\n",
            "Frequency Of--> citation -> 3\n",
            "Frequency Of--> bacon -> 1\n",
            "Frequency Of--> lawful -> 3\n",
            "Frequency Of--> at -> 17\n",
            "Frequency Of--> secured -> 1\n",
            "Frequency Of--> question -> 5\n",
            "Frequency Of--> maintain -> 1\n",
            "Frequency Of--> confirmation -> 1\n",
            "Frequency Of--> below -> 1\n",
            "Frequency Of--> annual -> 1\n",
            "Frequency Of--> other -> 9\n",
            "Frequency Of--> chattels -> 1\n",
            "Frequency Of--> page -> 1\n",
            "Frequency Of--> feel -> 1\n",
            "Frequency Of--> officer -> 3\n",
            "Frequency Of--> out -> 1\n",
            "Frequency Of--> synopsis -> 1\n",
            "Frequency Of--> prohibited -> 1\n",
            "Frequency Of--> authorities -> 1\n",
            "Frequency Of--> gathered -> 19\n",
            "Frequency Of--> firms -> 1\n",
            "Frequency Of--> paying -> 1\n",
            "Frequency Of--> sold -> 7\n",
            "Frequency Of--> are -> 9\n",
            "Frequency Of--> can -> 3\n",
            "Frequency Of--> frustrated -> 1\n",
            "Frequency Of--> acts -> 2\n",
            "Frequency Of--> should -> 3\n",
            "Frequency Of--> tried -> 6\n",
            "Frequency Of--> of -> 142\n",
            "Frequency Of--> controlled -> 1\n",
            "Frequency Of--> g -> 1\n",
            "Frequency Of--> sell -> 5\n",
            "Frequency Of--> partner -> 1\n",
            "Frequency Of--> authorize -> 1\n",
            "Frequency Of--> trover -> 3\n",
            "Frequency Of--> keeping -> 1\n",
            "Frequency Of--> seventyseven -> 1\n",
            "Frequency Of--> be -> 41\n",
            "Frequency Of--> growing -> 16\n",
            "Frequency Of--> plantation -> 1\n",
            "Frequency Of--> fa -> 1\n",
            "Frequency Of--> expressed -> 1\n",
            "Frequency Of--> indicated -> 1\n",
            "Frequency Of--> before -> 8\n",
            "Frequency Of--> short -> 1\n",
            "Frequency Of--> certain -> 1\n",
            "Frequency Of--> last -> 3\n",
            "Frequency Of--> matured -> 1\n",
            "Frequency Of--> wl -> 1\n",
            "Frequency Of--> lampley -> 1\n",
            "Frequency Of--> mansony -> 1\n",
            "Frequency Of--> m -> 2\n",
            "Frequency Of--> is -> 50\n",
            "Frequency Of--> appeal -> 6\n",
            "Frequency Of--> horton -> 1\n",
            "Frequency Of--> defendant -> 12\n",
            "Frequency Of--> doubted -> 1\n",
            "Frequency Of--> indicate -> 1\n",
            "Frequency Of--> suspend -> 1\n",
            "Frequency Of--> doubts -> 2\n",
            "Frequency Of--> on -> 33\n",
            "Frequency Of--> jones -> 2\n",
            "Frequency Of--> d -> 3\n",
            "Frequency Of--> held -> 1\n",
            "Frequency Of--> both -> 1\n",
            "Frequency Of--> their -> 6\n",
            "Frequency Of--> written -> 2\n",
            "Frequency Of--> temporarily -> 1\n",
            "Frequency Of--> contradict -> 1\n",
            "Frequency Of--> dec -> 1\n",
            "Frequency Of--> mistaken -> 1\n",
            "Frequency Of--> service -> 1\n",
            "Frequency Of--> reasons -> 1\n",
            "Frequency Of--> nov -> 2\n",
            "Frequency Of--> burton -> 1\n",
            "Frequency Of--> into -> 1\n",
            "Frequency Of--> yet -> 2\n",
            "Frequency Of--> elliott -> 1\n",
            "Frequency Of--> references -> 1\n",
            "Frequency Of--> bales -> 2\n",
            "Frequency Of--> where -> 2\n",
            "Frequency Of--> st -> 1\n",
            "Frequency Of--> till -> 1\n",
            "Frequency Of--> indorsers -> 6\n",
            "Frequency Of--> destroyed -> 1\n",
            "Frequency Of--> viz -> 1\n",
            "Frequency Of--> trespass -> 1\n",
            "Frequency Of--> it -> 80\n",
            "Frequency Of--> after -> 3\n",
            "Frequency Of--> ormond -> 1\n",
            "Frequency Of--> claim -> 1\n",
            "Frequency Of--> dewey -> 1\n",
            "Frequency Of--> b -> 2\n",
            "Frequency Of--> rees -> 1\n",
            "Frequency Of--> four -> 1\n",
            "Frequency Of--> nothing -> 3\n",
            "Frequency Of--> recovered -> 1\n",
            "Frequency Of--> merely -> 2\n",
            "Frequency Of--> aik -> 3\n",
            "Frequency Of--> disputed -> 1\n",
            "Frequency Of--> government -> 1\n",
            "Frequency Of--> him -> 2\n",
            "Frequency Of--> mere -> 3\n",
            "Frequency Of--> cannot -> 4\n",
            "Frequency Of--> mayfield -> 2\n",
            "Frequency Of--> good -> 4\n",
            "Frequency Of--> sealed -> 1\n",
            "Frequency Of--> require -> 1\n",
            "Frequency Of--> decided -> 1\n",
            "Frequency Of--> such -> 7\n",
            "Frequency Of--> bring -> 2\n",
            "Frequency Of--> who -> 2\n",
            "Frequency Of--> johns -> 7\n",
            "Frequency Of--> statue -> 1\n",
            "Frequency Of--> taking -> 1\n",
            "Frequency Of--> wash -> 1\n",
            "Frequency Of--> destruction -> 1\n",
            "Frequency Of--> forbids -> 1\n",
            "Frequency Of--> relate -> 1\n",
            "Frequency Of--> c -> 9\n",
            "Frequency Of--> sheriff -> 8\n",
            "Frequency Of--> indorsed -> 1\n",
            "Frequency Of--> lien -> 25\n",
            "Frequency Of--> giving -> 1\n",
            "Frequency Of--> immaterial -> 1\n",
            "Frequency Of--> majority -> 1\n",
            "Frequency Of--> cut -> 1\n",
            "Frequency Of--> inclined -> 1\n",
            "Frequency Of--> charge -> 3\n",
            "Frequency Of--> its -> 9\n",
            "Frequency Of--> cotton -> 14\n",
            "Frequency Of--> sawyer -> 2\n",
            "Frequency Of--> exemption -> 2\n",
            "Frequency Of--> unlawful -> 1\n",
            "Frequency Of--> trial -> 3\n",
            "Frequency Of--> potent -> 1\n",
            "Frequency Of--> levy -> 25\n",
            "Frequency Of--> no -> 16\n",
            "Frequency Of--> owes -> 1\n",
            "Frequency Of--> real -> 1\n",
            "Frequency Of--> put -> 2\n",
            "Frequency Of--> consider -> 2\n",
            "Frequency Of--> proposition -> 2\n",
            "Frequency Of--> satisfy -> 1\n",
            "Frequency Of--> referred -> 1\n",
            "Frequency Of--> enacts -> 1\n",
            "Frequency Of--> bond -> 1\n",
            "Frequency Of--> oats -> 1\n",
            "Frequency Of--> i -> 4\n",
            "Frequency Of--> lamar -> 1\n",
            "Frequency Of--> thirty -> 1\n",
            "Frequency Of--> remedies -> 1\n",
            "Frequency Of--> deliver -> 1\n",
            "Frequency Of--> clear -> 1\n",
            "Frequency Of--> bowman -> 1\n",
            "Frequency Of--> think -> 3\n",
            "Frequency Of--> connected -> 2\n",
            "Frequency Of--> save -> 1\n",
            "Frequency Of--> supreme -> 1\n",
            "Frequency Of--> deference -> 1\n",
            "Frequency Of--> when -> 5\n",
            "Frequency Of--> upon -> 11\n",
            "Frequency Of--> absent -> 1\n",
            "Frequency Of--> wheat -> 4\n",
            "Frequency Of--> supposes -> 1\n",
            "Frequency Of--> purpose -> 2\n",
            "Frequency Of--> exceptions -> 3\n",
            "Frequency Of--> quitclaimed -> 1\n",
            "Frequency Of--> years -> 1\n",
            "Frequency Of--> our -> 1\n",
            "Frequency Of--> silent -> 1\n",
            "Frequency Of--> grain -> 1\n",
            "Frequency Of--> coupled -> 2\n",
            "Frequency Of--> while -> 3\n",
            "Frequency Of--> for -> 27\n",
            "Frequency Of--> alston -> 1\n",
            "Frequency Of--> stated -> 1\n",
            "Frequency Of--> consequent -> 1\n",
            "Frequency Of--> pluries -> 1\n",
            "Frequency Of--> production -> 1\n",
            "Frequency Of--> requiring -> 2\n",
            "Frequency Of--> prevail -> 1\n",
            "Frequency Of--> defendantcited -> 1\n",
            "Frequency Of--> subjectmatter -> 1\n",
            "Frequency Of--> particular -> 1\n",
            "Frequency Of--> subsequently -> 1\n",
            "Frequency Of--> thirtyseven -> 1\n",
            "Frequency Of--> contended -> 1\n",
            "Frequency Of--> assign -> 1\n",
            "Frequency Of--> cases -> 3\n",
            "Frequency Of--> waiver -> 1\n",
            "Frequency Of--> says -> 3\n",
            "Frequency Of--> harrison -> 11\n",
            "Frequency Of--> cow -> 2\n",
            "Frequency Of--> original -> 1\n",
            "Frequency Of--> garnishment -> 1\n",
            "Frequency Of--> manner -> 1\n",
            "Frequency Of--> here -> 1\n",
            "Frequency Of--> opinion -> 4\n",
            "Frequency Of--> alias -> 1\n",
            "Frequency Of--> acquired -> 2\n",
            "Frequency Of--> undertook -> 1\n",
            "Frequency Of--> find -> 1\n",
            "Frequency Of--> gift -> 1\n",
            "Frequency Of--> whilst -> 1\n",
            "Frequency Of--> twentysecond -> 1\n",
            "Frequency Of--> united -> 1\n",
            "Frequency Of--> liable -> 1\n",
            "Frequency Of--> john -> 2\n",
            "Frequency Of--> saffold -> 1\n",
            "Frequency Of--> delivered -> 1\n",
            "Frequency Of--> personal -> 2\n",
            "Frequency Of--> thorough -> 1\n",
            "Frequency Of--> harvest -> 1\n",
            "Frequency Of--> results -> 3\n",
            "Frequency Of--> instructed -> 1\n",
            "Frequency Of--> cohen -> 2\n",
            "Frequency Of--> subject -> 8\n",
            "Frequency Of--> loss -> 1\n",
            "Frequency Of--> set -> 1\n",
            "Frequency Of--> mercantile -> 1\n",
            "Frequency Of--> upwards -> 2\n",
            "Frequency Of--> vested -> 1\n",
            "Frequency Of--> some -> 2\n",
            "Frequency Of--> continuing -> 2\n",
            "Frequency Of--> supposed -> 1\n",
            "Frequency Of--> murphy -> 1\n",
            "Frequency Of--> liability -> 3\n",
            "Frequency Of--> assumption -> 1\n",
            "Frequency Of--> writing -> 3\n",
            "Frequency Of--> with -> 15\n",
            "Frequency Of--> further -> 1\n",
            "Frequency Of--> was -> 54\n",
            "Frequency Of--> counts -> 1\n",
            "Frequency Of--> november -> 1\n",
            "Frequency Of--> judgment -> 7\n",
            "Frequency Of--> marshall -> 1\n",
            "Frequency Of--> nullity -> 1\n",
            "Frequency Of--> policy -> 1\n",
            "Frequency Of--> severance -> 3\n",
            "Frequency Of--> defeat -> 1\n",
            "Frequency Of--> has -> 13\n",
            "Frequency Of--> booker -> 1\n",
            "Frequency Of--> september -> 2\n",
            "Frequency Of--> might -> 3\n",
            "Frequency Of--> approved -> 1\n",
            "Frequency Of--> severing -> 1\n",
            "Frequency Of--> supports -> 1\n",
            "Frequency Of--> wages -> 1\n",
            "Frequency Of--> create -> 1\n",
            "Frequency Of--> fact -> 1\n",
            "Frequency Of--> possess -> 1\n",
            "Frequency Of--> creditor -> 2\n",
            "Frequency Of--> because -> 6\n",
            "Frequency Of--> october -> 1\n",
            "Frequency Of--> own -> 1\n",
            "Frequency Of--> parol -> 2\n",
            "Frequency Of--> equity -> 2\n",
            "Frequency Of--> hale -> 2\n",
            "Frequency Of--> acres -> 1\n",
            "Frequency Of--> raised -> 2\n",
            "Frequency Of--> adm -> 1\n",
            "Frequency Of--> vary -> 1\n",
            "Frequency Of--> contract -> 16\n",
            "Frequency Of--> grantor -> 2\n",
            "Frequency Of--> postpone -> 2\n",
            "Frequency Of--> away -> 4\n",
            "Frequency Of--> n -> 1\n",
            "Frequency Of--> principle -> 1\n",
            "Frequency Of--> competent -> 1\n",
            "Frequency Of--> matter -> 3\n",
            "Frequency Of--> learned -> 1\n",
            "Frequency Of--> twenty -> 1\n",
            "Frequency Of--> future -> 3\n",
            "Frequency Of--> will -> 8\n",
            "Frequency Of--> employed -> 1\n",
            "Frequency Of--> document -> 1\n",
            "Frequency Of--> conveyance -> 1\n",
            "Frequency Of--> weakley -> 1\n",
            "Frequency Of--> seizure -> 2\n",
            "Frequency Of--> singular -> 1\n",
            "Frequency Of--> plaintiff -> 14\n",
            "Frequency Of--> parties -> 1\n",
            "Frequency Of--> my -> 2\n",
            "Frequency Of--> end -> 1\n",
            "Frequency Of--> naked -> 1\n",
            "Frequency Of--> treatment -> 4\n",
            "Frequency Of--> gives -> 1\n",
            "Frequency Of--> satisfied -> 2\n",
            "Frequency Of--> hands -> 3\n",
            "Frequency Of--> cites -> 1\n",
            "Frequency Of--> sum -> 1\n",
            "Frequency Of--> salk -> 2\n",
            "Frequency Of--> ravesies -> 1\n",
            "Frequency Of--> conceding -> 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR0L3_CreM_A",
        "outputId": "bcd8ed81-162a-4335-9778-34e652e080f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "#Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\n",
        "#top 10, 1-gram\n",
        "from itertools import chain\n",
        "from nltk import ngrams\n",
        "new=[]\n",
        "for i in words:\n",
        "  tks = word_tokenize(i)\n",
        "  new.append(tks)\n",
        "list2 = [x for x in new if x != []]\n",
        "a=list(itertools.chain.from_iterable(list2))\n",
        "#one =ngrams(a, n)\n",
        "one=ngrams(a,1)\n",
        "print('Top 10 one grams-')\n",
        "Counter(one).most_common(10)\n"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 one grams-\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('the',), 337),\n",
              " (('of',), 142),\n",
              " (('to',), 116),\n",
              " (('a',), 84),\n",
              " (('and',), 83),\n",
              " (('in',), 81),\n",
              " (('it',), 80),\n",
              " (('that',), 56),\n",
              " (('was',), 54),\n",
              " (('execution',), 50)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFtucom8mQjr",
        "outputId": "2bf5aa2d-5597-445f-c79b-a6dd6ffc7232",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "two_grms = nltk.bigrams(a)\n",
        "fdist = nltk.FreqDist(two_grms)\n",
        "print(\"Top 10 2-grams\")\n",
        "(fdist.most_common(10))"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 2-grams\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('of', 'the'), 57),\n",
              " (('the', 'crop'), 25),\n",
              " (('to', 'the'), 23),\n",
              " (('the', 'claimants'), 21),\n",
              " (('that', 'the'), 20),\n",
              " (('an', 'execution'), 17),\n",
              " (('it', 'is'), 17),\n",
              " (('in', 'execution'), 17),\n",
              " (('the', 'right'), 16),\n",
              " (('in', 'the'), 16)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPiS8r_wmrSl",
        "outputId": "0e179e77-5a23-4b59-8f70-a3864b1d7f15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "print(\"Top 10 3-grams\")\n",
        "tri = nltk.trigrams(a)\n",
        "fdist = nltk.FreqDist(tri)\n",
        "fdist.most_common(10)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 3-grams\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('the', 'defendant', 'in'), 11),\n",
              " (('defendant', 'in', 'execution'), 11),\n",
              " (('the', 'right', 'to'), 10),\n",
              " (('the', 'circuit', 'court'), 8),\n",
              " (('of', 'the', 'crop'), 8),\n",
              " (('circuit', 'court', 'of'), 7),\n",
              " (('until', 'the', 'crop'), 7),\n",
              " (('a', 'growing', 'crop'), 6),\n",
              " (('of', 'the', 'plaintiff'), 6),\n",
              " (('right', 'to', 'levy'), 6)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBiC4E_kefvV"
      },
      "source": [
        "# 2. Python Regular Expression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1QJ-UwCenvN"
      },
      "source": [
        "## 2.1 Write a Python program to remove leading zeros from an IP address. (4 points)\n",
        "\n",
        "ip = \"260.08.094.109\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSv6fVhOfFmv",
        "outputId": "3fef9dfd-95a7-47e8-a171-47a0cd219a47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# function to remove leading zeros \n",
        "def removeZeros(ip): \n",
        "    new_ip = \".\".join([str(int(i)) for i in ip.split(\".\")])   \n",
        "    return new_ip ; \n",
        "ip =\"260.08.094.109\"  \n",
        "print(removeZeros(ip)) \n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "260.8.94.109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXRjaHzrfKAy"
      },
      "source": [
        "## 2.2 Write a Python Program to extract all the years from the following sentence. (4 points)\n",
        "\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xdJpDx9gjbX",
        "outputId": "10e1b143-554a-4b2e-9088-60dda04d0f83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import re\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\"\n",
        "result=re.findall(r'2\\d\\d\\d', sentence)\n",
        "result"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2010', '2010', '2019']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfE8yktMLFLk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}